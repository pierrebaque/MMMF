{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.eigen as e\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# sys.path.insert(0, '../CRFRNN/crfasrnn/caffe-crfrnn/python/')\n",
    "# sys.path.insert(0, '../CRFRNN/crfasrnn/train-CRF-RNN/')\n",
    "\n",
    "\n",
    "# from py_img_seg_eval.eval_segm import *\n",
    "# from utils import get_id_classes, create_lut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "\"prepare test list\"\n",
    "\n",
    "\n",
    "image_dir = \"/cvlabdata1/home/baque/python/deep_learning/data/datasets/VOCdevkit/VOC2012/JPEGImages/\"\n",
    "test_image_list_file = \"/cvlabdata1/home/baque/python/deep_learning/data/datasets/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt\"\n",
    "label_dir = \"/cvlabdata1/home/baque/python/CRFRNN/crfasrnn/train-CRF-RNN/converted_labels/\" \n",
    "file_names = []\n",
    "label_names =[]\n",
    "\n",
    "with open(test_image_list_file, 'rb') as f:\n",
    "    for fn in f:\n",
    "        file_names.append(image_dir + fn.strip()+\".jpg\")\n",
    "        label_names.append(label_dir + fn.strip()+\".png\")\n",
    "        \n",
    "pallete = [0,0,0,\n",
    "            128,0,0,\n",
    "            0,128,0,\n",
    "            128,128,0,\n",
    "            0,0,128,\n",
    "            128,0,128,\n",
    "            0,128,128,\n",
    "            128,128,128,\n",
    "            64,0,0,\n",
    "            192,0,0,\n",
    "            64,128,0,\n",
    "            192,128,0,\n",
    "            64,0,128,\n",
    "            192,0,128,\n",
    "            64,128,128,\n",
    "            192,128,128,\n",
    "            0,64,0,\n",
    "            128,64,0,\n",
    "            0,192,0,\n",
    "            128,192,0,\n",
    "            0,64,128,\n",
    "            128,64,128,\n",
    "            0,192,128,\n",
    "            128,192,128,\n",
    "            64,64,0,\n",
    "            192,64,0,\n",
    "            64,192,0,\n",
    "            192,192,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN Z (CNMeM is disabled, cuDNN 5110)\n",
      "/home/baque/.local/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/home/baque/.local/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# We use Theano to do average-filtering efficiently, but any other method can be used\n",
    "from theano import *\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.cuda import dnn\n",
    "\n",
    "import theano\n",
    "import pandas as pd\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "import cPickle, gzip, numpy\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.downsample import max_pool_2d\n",
    "stride =8\n",
    "\n",
    "kernel_size =T.scalar('size', dtype='int32')\n",
    "in_img = T.tensor4('in', dtype='float64')\n",
    "kernel = T.alloc(1.0,1,1,kernel_size,kernel_size)/(kernel_size*kernel_size)\n",
    "\n",
    "H,W = in_img.shape[2:]\n",
    "\n",
    "convolved = conv2d(in_img,kernel,border_mode = 'full',subsample =(stride,stride))[:,:,kernel_size//(2*stride):kernel_size//(2*stride)+H//stride,kernel_size//(2*stride):kernel_size//(2*stride)+W//stride]\n",
    "\n",
    "convolution_function = theano.function(inputs=[in_img ,kernel_size], outputs=[ convolved ], updates=[], allow_input_downcast=True,on_unused_input='warn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "import copy\n",
    "\n",
    "\n",
    "sxy_g=3 \n",
    "compat_g=3\n",
    "sxy_b=2\n",
    "srgb=3\n",
    "compat_b=10\n",
    "\n",
    "\n",
    "\n",
    "def perform_inference_Lagrange(U,img_in,clamps,n_steps = 5,T = 1.0,verbose = False,step =1.0,N=1000,proportion = 0.5,returnKL = False):\n",
    "    H,W = img_in.shape[:2]\n",
    "    NC = U.shape[0]\n",
    "    d = dcrf.DenseCRF2D(H, W, NC)\n",
    "    \n",
    "    T_i = T*2\n",
    "    d.setUnaryEnergy(U/(T_i))\n",
    "    d.addPairwiseGaussian(sxy=sxy_g, compat=compat_g/(T_i))\n",
    "    d.addPairwiseBilateral(sxy=sxy_b, srgb=srgb, rgbim=img_in, compat=compat_b/(T_i))  \n",
    "    \n",
    "    #Initialise Q\n",
    "    Q_out, tmp1, tmp2 = d.startInference()\n",
    "    Q = np.asarray(Q_out)*0.0 + 1.0\n",
    "    Q = np.exp(np.log(Q_out)*step +(1-step)*np.log(Q))\n",
    "    Q = Q/ np.sum(Q,axis =0)\n",
    "    \n",
    "    #Initialise Lagrange multipliers\n",
    "    Lambdas = np.zeros(len(clamps)) +1\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        \n",
    "                    \n",
    "        ### Option 2\n",
    "        HO_grad = 0#useless\n",
    "        Lagrange_grad = get_Lagrange_grad(Lambdas,Q,clamps)\n",
    "        if i == n_steps/5:\n",
    "            T_i = T\n",
    "            d = dcrf.DenseCRF2D(H, W, NC)\n",
    "            d.addPairwiseGaussian(sxy=sxy_g, compat=compat_g/(T_i))\n",
    "            d.addPairwiseBilateral(sxy=sxy_b, srgb=srgb, rgbim=img_in, compat=compat_b/(T_i))  \n",
    "            \n",
    "        \n",
    "        d.setUnaryEnergy(U/T_i - Lagrange_grad)\n",
    "\n",
    "        \n",
    "        Q_out = copy.copy(Q.copy(order='C'))\n",
    "        Q_out = e.matrixXf(Q_out)\n",
    "        d.stepInference(Q_out, tmp1, tmp2)\n",
    "        if verbose:\n",
    "            print(\"KL-divergence at {}: {}\".format(i, d.klDivergence(Q_out)))\n",
    "\n",
    "        Q = np.exp(np.log(Q_out)*step +(1-step)*np.log(Q))\n",
    "        Q = Q/ np.sum(Q,axis = 0)\n",
    "        \n",
    "        Lambdas = update_Lagrange(Lambdas,Q,clamps,proportion)\n",
    "        \n",
    "    #FIne tune without external\n",
    "    for i in range(3):\n",
    "        d.setUnaryEnergy(U/T_i)\n",
    "        Q_out = copy.copy(Q.copy(order='C'))\n",
    "        Q_out = e.matrixXf(Q_out)\n",
    "        d.stepInference(Q_out, tmp1, tmp2)\n",
    "        Q = np.asarray(Q_out)\n",
    "\n",
    "\n",
    "    if returnKL:\n",
    "        KL = d.klDivergence(Q_out)\n",
    "        return np.asarray(Q_out).reshape((-1,H,W)),KL\n",
    "\n",
    "    else:\n",
    "        return np.asarray(Q_out).reshape((-1,H,W))\n",
    "\n",
    "\n",
    "def update_Lagrange(Lambdas,Q,clamps,proportion ,Lambda_step = 5.0):\n",
    "    for l,(variables,initial_values,sign) in enumerate(clamps):\n",
    "        N = proportion*len(variables)\n",
    "        q_neg = 1-Q[initial_values,variables]\n",
    "        Lambdas[l] = np.clip(Lambdas[l] + Lambda_step *(N-np.sum(q_neg))/(len(variables)*1.0)*sign ,2.8,8.0)\n",
    "    \n",
    "    return Lambdas\n",
    "\n",
    "def get_Lagrange_grad(Lambdas,Q,clamps,Lambda_step = 10):\n",
    "    grad = 0.0*Q\n",
    "    for l,(variables,initial_values,sign) in enumerate(clamps):\n",
    "        #grad[:,variables] += Lambdas[l]*sign\n",
    "        grad[initial_values,variables] += -1*Lambdas[l]*sign\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_variables_multiscale(Q_low,Q_high,clamps):\n",
    "\n",
    "    indicators = 0.0*Q_low+1\n",
    "    n_labels,H,W = Q_low.shape\n",
    "    \n",
    "    #Filter-out the ones which have low Q_low\n",
    "    filter_low = Q_low<0.9\n",
    "    indicators[filter_low] = 0.0\n",
    "    \n",
    "    #TEST, uncomment\n",
    "    for (variables,initial_values,sign) in clamps:\n",
    "        indicators[initial_values,variables/W,variables%W] =0.0\n",
    "\n",
    "################\n",
    "    \n",
    "    \n",
    "    #Compute change between low and high temp\n",
    "    Tchange_perclass = (Q_low/Q_high-1)*indicators\n",
    "    Tchange_perclass[Tchange_perclass>2] = 2\n",
    "\n",
    "    #remove the border\n",
    "    Tchange_perclass[:,0:20,:] =0\n",
    "    Tchange_perclass[:,H-20:H,:] =0\n",
    "    Tchange_perclass[:,:,0:20] =0\n",
    "    Tchange_perclass[:,:,W-20:W] =0\n",
    "\n",
    "    \n",
    "    kernel_list =[(21,21),(51,51),(91,91),(151,151)]\n",
    "    all_convolved = []\n",
    "    Tchange_perclass_theano = Tchange_perclass.reshape((-1,1,H,W))\n",
    "    \n",
    "    #Show\n",
    "    \n",
    "    for k,(h_k,w_k) in enumerate(kernel_list):    \n",
    "        convolved = np.asarray(convolution_function(Tchange_perclass_theano,h_k))[0,:,0,:,:]*(h_k*w_k)**(0.25)\n",
    "        all_convolved.append(convolved)\n",
    "              \n",
    "    #Select window and label\n",
    "    max_value = 0\n",
    "    k = 0\n",
    "    l, x_c, y_c =(0,0,0)\n",
    "    for loc_k,(h_k,w_k) in enumerate(kernel_list):\n",
    "        arg_index =np.argmax(all_convolved[loc_k])\n",
    "        loc_l, loc_x_c, loc_y_c =np.unravel_index(arg_index, all_convolved[loc_k].shape)\n",
    "                \n",
    "        if all_convolved[loc_k][loc_l, loc_x_c, loc_y_c] > max_value:\n",
    "            max_value = all_convolved[loc_k][loc_l, loc_x_c, loc_y_c]\n",
    "            l, x_c, y_c = loc_l, loc_x_c, loc_y_c\n",
    "            k = loc_k\n",
    "        \n",
    "    #Puting back to coordinates without stride\n",
    "    x_c, y_c =  stride*x_c, stride*y_c\n",
    "    \n",
    "     \n",
    "    MAP = np.argmax(Q_low, axis=0).reshape((H,W))\n",
    "        \n",
    "    #values\n",
    "    x_0,y_0 = max(0,x_c-h_k/2),max(0,y_c-w_k/2)\n",
    "    x_1,y_1 = min(H,x_c+h_k/2+1),min(W,y_c+w_k/2+1)\n",
    "    \n",
    "    initial_values = np.ndarray.flatten(MAP[x_0 :x_1, y_0 :y_1])\n",
    "    \n",
    "    #Indices\n",
    "    y_variables = np.repeat(np.asarray(range(y_0 ,y_1)).reshape(y_1-y_0,1),x_1-x_0,axis =1)\n",
    "    x_variables = np.repeat(W*np.asarray(range(x_0 ,x_1)).reshape(x_1-x_0,1),y_1-y_0,axis =1)\n",
    "    xy_variables = x_variables +y_variables.T\n",
    "    variables = np.ndarray.flatten(xy_variables)#[0:min(h_k,H - (x_c-h_k/2) ),0:min(w_k,W - (y_c-w_k/2))])\n",
    "    \n",
    "    #Now, only select the ones that initialy took label l\n",
    "    variables = variables[initial_values==l]\n",
    "    initial_values = initial_values[initial_values==l]\n",
    "    \n",
    " \n",
    "    return variables, initial_values,x_c,y_c,h_k,w_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_clamplist(initial_clamplist,variables,initial_values,index):\n",
    "    \n",
    "\n",
    "    clamplist = copy.copy(initial_clamplist)\n",
    "    l = len(clamplist)\n",
    "    if l ==0:\n",
    "        clamplist=[[(variables,initial_values,-1)],[(variables,initial_values,1)]]\n",
    "    else:\n",
    "        current_clamps = copy.copy(clamplist[index])\n",
    "        del clamplist[index]\n",
    "        clamplist.insert(index,current_clamps + [(variables,initial_values,-1)])\n",
    "        clamplist.insert(index+1,current_clamps + [(variables,initial_values,1)])\n",
    "    return clamplist\n",
    "\n",
    "def run_example(fid,n_nodes = 31):\n",
    "\n",
    "\n",
    "    #Load unaries\n",
    "    f = open('./data_example/%06d_unary.cpickle'%0, 'rb')\n",
    "    in_unaries = pickle.load(f)\n",
    "    f.close()\n",
    "   \n",
    "    in_unaries = np.squeeze(in_unaries)\n",
    "\n",
    "\n",
    "\n",
    "    NC = in_unaries.shape[0]\n",
    "\n",
    "    in_rgb_original =Image.open('./data_example/%06d_rgb.jpg'%0)\n",
    "\n",
    "    #Test pyDenseCRF\n",
    "    #Initialisation\n",
    "    img_in =copy.copy(np.asarray(in_rgb_original))\n",
    "    H,W = img_in.shape[:2]\n",
    "    U = -1*in_unaries[:,:H,:W].reshape((NC,-1)) # Needs to be flat.\n",
    "    U = U.copy(order='C')\n",
    "    U[0,:] =U[0,:]\n",
    "\n",
    "\n",
    "\n",
    "    #Loop\n",
    "    index =0\n",
    "    clamplist =[]\n",
    "\n",
    "    for n in range(0,n_nodes):\n",
    "        #print \"Node # %02d\"%n\n",
    "        #print \"clamplist\", clamplist\n",
    "\n",
    "        if len(clamplist)==0:\n",
    "            Q_high = perform_inference_Lagrange(U,img_in,[],n_steps = 8,T=5.0,verbose=False,step =1.0)\n",
    "            Q_low = perform_inference_Lagrange(U,img_in,[],n_steps = 8,T=1,verbose=False,step =1.0)\n",
    "            variables, initial_values,x_c,y_c,h_k,w_k = select_variables_multiscale(Q_low,Q_high,[])\n",
    "            MAP_0 = np.argmax(Q_low, axis=0).reshape((H,W))\n",
    "\n",
    "\n",
    "        if len(clamplist)!=0:\n",
    "            index = (index +2)%(len(clamplist))\n",
    "            #print \"Index\", index,'len clamplist', len(clamplist)\n",
    "            Q_high = perform_inference_Lagrange(U,img_in,clamplist[index],n_steps = 8,T=5.0,verbose=False,step =1.0)\n",
    "            Q_low = perform_inference_Lagrange(U,img_in,clamplist[index],n_steps = 8,T=1,verbose=False,step =1.0)\n",
    "            variables, initial_values,x_c,y_c,h_k,w_k = select_variables_multiscale(Q_low,Q_high,clamplist[index])\n",
    "\n",
    "\n",
    "        clamplist = add_clamplist(clamplist,variables,initial_values,index)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Compute IU in final nodes\n",
    "    \n",
    "    Q_list = []\n",
    "    MAP_list =[]\n",
    "    for index in range(len(clamplist)):\n",
    "        Q_low,KL = perform_inference_Lagrange(U,img_in,clamplist[index],n_steps = 8,T=1,verbose=False,step =1.0,returnKL=True)\n",
    "\n",
    "        pred = np.argmax(Q_low, axis=0).reshape((H,W))\n",
    "\n",
    "        MAP_list.append(pred)\n",
    "        Q_list.append(Q_low)\n",
    "            \n",
    "\n",
    "    return MAP_list,Q_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
